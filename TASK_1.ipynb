{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "o3B5XHyVZmma",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3B5XHyVZmma",
    "outputId": "b5038ffb-bc1e-4326-90e0-2a7eb439c4b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model f1 is completed\n",
      "Model f2 is completed\n",
      "Model f3 is completed\n",
      "Model f4 is completed\n",
      "Model f5 is completed\n",
      "Model f6 is completed\n",
      "Model f7 is completed\n",
      "Model f8 is completed\n",
      "Model f9 is completed\n",
      "Model f10 is completed\n",
      "Evaluation of model1 done!\n",
      "Evaluation of model2 done!\n",
      "Evaluation of model3 done!\n",
      "Evaluation of model4 done!\n",
      "Evaluation of model5 done!\n",
      "Evaluation of model6 done!\n",
      "Evaluation of model7 done!\n",
      "Evaluation of model8 done!\n",
      "Evaluation of model9 done!\n",
      "Evaluation of model10 done!\n",
      "Accuracy Matrix (Rows: Models f1 to f10, Columns: Held-out datasets D1 to D10):\n",
      "99.96\t\n",
      "99.96\t86.88\t\n",
      "99.96\t86.96\t87.32\t\n",
      "99.96\t86.84\t87.16\t87.48\t\n",
      "99.96\t86.80\t87.32\t87.40\t87.44\t\n",
      "99.96\t86.72\t87.44\t87.16\t87.52\t87.36\t\n",
      "99.96\t86.72\t87.40\t87.16\t87.48\t87.40\t86.24\t\n",
      "99.96\t86.64\t87.20\t87.12\t87.32\t87.36\t86.16\t86.92\t\n",
      "99.96\t86.64\t87.28\t87.20\t87.44\t87.32\t86.20\t86.88\t86.12\t\n",
      "99.96\t86.64\t87.20\t87.04\t87.32\t87.60\t86.04\t86.96\t86.00\t87.56\t\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Setting random seed for getting same accuracy matrix every time\n",
    "seed = 45\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    features = data['features']   # storing features\n",
    "    if 'targets' in data:\n",
    "        targets = data['targets'] #storing targets\n",
    "        targets = targets.clone().detach() if isinstance(targets, torch.Tensor) else torch.tensor(targets, dtype=torch.long)\n",
    "    else:          # targets is None if data dont have targets\n",
    "        targets = None\n",
    "    return features, targets\n",
    "\n",
    "# Function to print accuracy matrix\n",
    "def print_accuracy_matrix(accuracy_matrix):\n",
    "    n = len(accuracy_matrix)\n",
    "    m = len(accuracy_matrix[0])\n",
    "    for i in range(n):\n",
    "        for j in range(i+1):\n",
    "            print(f\"{accuracy_matrix[i][j]:.2f}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "# Initial LwP model training function using hard LWP model i.e each point belong to 1 class dont consider probability of belonging to other class\n",
    "def train_lwp_hard(features, targets, regularization=1e-2, prototype_smoothing=0.2, noise_std=0.1):\n",
    "    num_classes = 10\n",
    "    prototypes = []  #storing prototype/mean of each class\n",
    "    cov_matrices = []  #storing covariance of each class\n",
    "    global_mean = features.mean(axis=0) # calculating global mean\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # Add Gaussian noise to the features of each class\n",
    "        class_features = features[targets == i]\n",
    "        noisy_class_features = class_features + np.random.normal(0, noise_std, class_features.shape)\n",
    "\n",
    "        # smoothing the prototype with global mean\n",
    "        prototype = (1 - prototype_smoothing) * noisy_class_features.mean(axis=0) + prototype_smoothing * global_mean\n",
    "        prototypes.append(prototype)\n",
    "\n",
    "        # covariance matrix regularizing\n",
    "        cov_matrix = np.cov(noisy_class_features, rowvar=False) + regularization * np.eye(class_features.shape[1])\n",
    "        cov_matrices.append(np.linalg.inv(cov_matrix))\n",
    "\n",
    "    return np.array(prototypes), cov_matrices\n",
    "\n",
    "\n",
    "# LwP model training function with soft clustering and progressive prototypes averaging i.e each points have probability of beloning to each class\n",
    "def train_lwp_soft(features, prev_prototypes=None, prev_cov_matrices=None, alpha=0.5, num_classes=10):\n",
    "    prototypes = []      #list to store prototypes/means        \n",
    "    cov_matrices = []   #list to store covariance matrices\n",
    "\n",
    "    # Compute soft assignments z_nk for each data point and each class\n",
    "\n",
    "    soft_assignments = np.zeros((features.shape[0], num_classes))\n",
    "    epsilon = 1e-12  # small value is taken to avoid division by zero error\n",
    "\n",
    "    for k in range(num_classes): #calculating distances to each class\n",
    "        distances = np.linalg.norm(features - prev_prototypes[k], axis=1) ** 2\n",
    "        soft_assignments[:, k] = np.exp(-distances) \n",
    "\n",
    "    # add epsilon to soft_assignment_sum to avoid division by zero error\n",
    "    soft_assignments_sum = soft_assignments.sum(axis=1, keepdims=True) + epsilon\n",
    "    soft_assignments /= soft_assignments_sum #calcuate probability of belonging to each class     \n",
    "\n",
    "    \n",
    "    for k in range(num_classes): # Update prototype (mean) with soft assignments\n",
    "        weighted_sum = np.sum(soft_assignments[:, k][:, np.newaxis] * features, axis=0)\n",
    "        total_weight = np.sum(soft_assignments[:, k])\n",
    "\n",
    "        # check whether total_weight > 0 or not\n",
    "        if total_weight > 0:\n",
    "            prototype = weighted_sum / total_weight\n",
    "        else:\n",
    "            # If total_weight is 0 use previous protype or use mean of features based on availability of prev_prototypes\n",
    "            prototype = prev_prototypes[k] if prev_prototypes is not None else np.mean(features, axis=0)\n",
    "\n",
    "        # update prototype based on given alpha value new prototype is weighted sum of current prototype and previous prototype\n",
    "        prototype = alpha * prototype + (1 - alpha) * prev_prototypes[k]\n",
    "        prototypes.append(prototype)\n",
    "\n",
    "        # update covariance matrix with soft assignments\n",
    "        cov_matrix = np.zeros((features.shape[1], features.shape[1]))\n",
    "        for n in range(features.shape[0]):\n",
    "            diff = (features[n] - prototype).reshape(-1, 1)\n",
    "            cov_matrix += soft_assignments[n, k] * (diff @ diff.T)\n",
    "        if total_weight > 0:\n",
    "            cov_matrix /= total_weight\n",
    "        else:\n",
    "            cov_matrix = np.cov(features, rowvar=False) + np.eye(features.shape[1]) * 1e-6  # Use feature covariance as fallback\n",
    "\n",
    "        # update covariance matrix based on given alpha values new covariance matrix is weighted sum of current covariance matrix and previous covariance matrix\n",
    "        cov_matrix = alpha * cov_matrix + (1 - alpha) * np.linalg.inv(prev_cov_matrices[k])\n",
    "        cov_matrices.append(np.linalg.inv(cov_matrix + np.eye(cov_matrix.shape[0]) * 1e-6))  #adding small value for stability purpose\n",
    "\n",
    "    return np.array(prototypes), cov_matrices\n",
    "\n",
    "\n",
    "#prediction function\n",
    "def predict_lwp(features, prototypes, cov_matrices):\n",
    "    predictions = []  # list to store predictions   \n",
    "    for feature in features:\n",
    "        distances = []  # list to store distance with respect to each class   \n",
    "        for i in range(len(prototypes)):\n",
    "            diff = feature - prototypes[i]\n",
    "            mah_dist = np.sqrt(diff.T @ cov_matrices[i] @ diff)  # calculating mahalanobis distance\n",
    "            distances.append(mah_dist)\n",
    "        predicted_class = np.argmin(distances) #gives class which has less distance to the given feature\n",
    "        predictions.append(predicted_class)\n",
    "    return predictions  # returning predictions\n",
    "\n",
    "\n",
    "#paths for the datasets\n",
    "dataset_paths = [fr'/content/drive/MyDrive/M TECH/ML/mini-project-2/dataset/v/only_features/extracted_features_{i}_data.tar.pkl' for i in range(1, 11)]\n",
    "heldout_paths = [fr'/content/drive/MyDrive/M TECH/ML/mini-project-2/dataset/v/extracted_features_{i}_data.tar.pkl' for i in range(1, 11)]\n",
    "\n",
    "\n",
    "models = []  # a list to store means and covariance matrices for each model\n",
    "accuracy_matrix = np.zeros((10, 10))  # a matrix to store accuracy of each model vs heldout datasets\n",
    "\n",
    "# Training first model f1 on D1 using hard classification\n",
    "features, targets = load_data(dataset_paths[0]) #loading features and targets\n",
    "#get prototypes and covariance matrix for model 1 from train_lwp_hard function\n",
    "prototypes, cov_matrices = train_lwp_hard(features, targets, regularization=1, prototype_smoothing=0.2)\n",
    "models.append((prototypes, cov_matrices)) #appending means and covariance matrix to models list\n",
    "print(\"Model f1 is completed\")\n",
    "\n",
    "\n",
    "#Updating models f2 to f10 using soft classification\n",
    "for i in range(1, 10):\n",
    "    features, _ = load_data(dataset_paths[i]) #loading features\n",
    "    # get prototypes and covariance matrices for model i+1 from train_lwp_soft function\n",
    "    prototypes, cov_matrices = train_lwp_soft(features, prev_prototypes=models[-1][0], prev_cov_matrices=models[-1][1], alpha=0.2)\n",
    "    models.append((prototypes, cov_matrices)) #appending means and covariance matrix to models list\n",
    "    print(f\"Model f{i+1} is completed\")\n",
    "\n",
    "# Evaluation of models on held out datasets\n",
    "for i, (prototypes, cov_matrices) in enumerate(models): #taking  model f1 to f10 iteratively\n",
    "    for j in range(i + 1):   #taking data set 1 to data set i for calculating goodness of i th  model on them\n",
    "        heldout_features, heldout_targets = load_data(heldout_paths[j]) #taking heldout features and targets\n",
    "        heldout_predictions = predict_lwp(heldout_features, prototypes, cov_matrices) #get predictions from predict_lwp function\n",
    "        accuracy = accuracy_score(heldout_targets.numpy(), heldout_predictions)\n",
    "        accuracy_matrix[i, j] = accuracy * 100  # storing accuracy in terms of 0-100 range\n",
    "    print(f\"Evaluation of model{i+1} done!\")\n",
    "\n",
    "# Print accuracy matrix\n",
    "print(\"Accuracy Matrix (Rows: Models f1 to f10, Columns: Held-out datasets D1 to D10):\")\n",
    "print_accuracy_matrix(accuracy_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "v50prtyN7JjM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v50prtyN7JjM",
    "outputId": "6080182c-86fc-45c0-84fc-e84356d41dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[228   2   1   1   1   0   1   0   7   3]\n",
      " [  4 226   0   0   0   0   1   0   1  15]\n",
      " [ 10   1 202   6   6   4   4   1   0   0]\n",
      " [  2   1   3 211   5  31  16   0   0   1]\n",
      " [  2   1   5   6 189   1  12   3   0   0]\n",
      " [  2   0   0  27   2 212   2   6   0   0]\n",
      " [  0   0   8  15   3   6 220   0   0   0]\n",
      " [  2   2   1   6   7   8   1 211   1   2]\n",
      " [  9   6   0   1   0   0   0   0 264   4]\n",
      " [  5  20   2   3   0   0   0   0   2 226]]\n"
     ]
    }
   ],
   "source": [
    "# printing Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(heldout_targets, heldout_predictions)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
